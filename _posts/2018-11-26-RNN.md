---
layout:     post
title:      循环神经网络RNN
subtitle:  RNN
date:       2018-12-24
author:     WJY
header-img: img/01.jpg
catalog: true
mathjax: true
tags:

    - 深度学习

    -  nlp
---





## 为什么要引入RNN？

传统神经网络DNN和CNN，输入和输出相互独立。如图像上猫和狗是分隔开的，但有些任务后面的输出和之前的内容相关，比如完形填空。

循环神经网络RNN中“循环”的意义是每个元素都执行相同的任务

RNN的输出依赖”输入“和”记忆“

## RNN结构图

![](https://ws4.sinaimg.cn/large/006tKfTcly1g0f1izby0vj31re0pkk0u.jpg)



$X_{t}$是时间t的输入

$S_{t}$是时间t的“记忆”，$S_{t}=f(UX_{t}+WS_{t-1})$,f可以是tanh函数

$O_{t}$是时间t的输出，比如是预测下个词的话，可能是softmax输出的属于每个候选词的概率，$O_{t}=softmax(VS_{t})$

### RNN结构细节

$S_{t}$ 是“记忆体”，捕捉了之前时间点上的信息；

输出$O_{t}$由当前时间以及之前所有“记忆”共同计算得到；

实际应用中，因为记忆有限，$S_{t}$ 并不能捕捉和保留之前所有信息；

不同于CNN，这里的RNN在整个神经网络共享一组参数（U、V、W），极大减少需要训练和预估的参数量；

图中$O_{t}$在有些任务是不存在的，比如文本情感分析，其实只需要在最后的output输出。

## 双向RNN

有些情况下，当前输出不止依赖之前序列还依赖之后序列元素，如完形填空，我们需要考虑上下文信息。

### 双向RNN结构图

![](https://ws3.sinaimg.cn/large/006tKfTcly1g0f3919kktj319g0fswk6.jpg)



U是concat

## 深层双向RNN

和双向RNN区别是每个时间步有多层结构

![](https://ws2.sinaimg.cn/large/006tKfTcly1g0f3bvv7d8j318q0lsqca.jpg)

i表示第几层

## BPTT算法

BPTT算法和BP是一个思路，只不过和时间步有关。

![](https://ws1.sinaimg.cn/large/006tKfTcly1g0f47psoqzj30xy0luq7j.jpg)







![](https://ws1.sinaimg.cn/large/006tKfTcly1g0f48fnre2j30wy0logpv.jpg)





## 小结

- RNN解决什么问题？

  输出和之前内容相关的问题

- RNN两大问题：梯度消失、梯度爆炸

  梯度爆炸可以用梯度裁剪方法解决；梯度消失解决方法：GRU、LSTM

  