---

layout:     post
title:      LSTM
subtitle:   RNN的变种
date:       2018-12-1
author:     WJY
header-img: img/01.jpg
catalog: true
tags:

- 深度学习


---
## LSTM

lstm是RNN的一种变体，大致结构一样，区别是：

- “记忆”改造了；
- 该记的信息会一直传递，不该记的会被“门”截断。

### LSTM结构图



![](https://ws4.sinaimg.cn/large/006tKfTcly1g0f4jpvoovj30x40gk0ws.jpg)

LSTM关键：“细胞状态”

细胞状态类似于传送带。直接在整个链上运行，只有一些少量的线性交互。信息在上面流传

保持不变会很容易。

![](https://ws4.sinaimg.cn/large/006tKfTcly1g0f4psgkydj30nk0cudhb.jpg)

### LSTM怎么控制“细胞状态”？

通过“门”让信息选择性通过，来去除或增加信息到细胞状态

包含一个sigmoid神经网络层和一个pointwise乘法操作

sigmoid层输出0和1之间的概率值，描述每个部分有多少量可以通过。0表示“不许任何量通过”，1表示“允许任意量通过”。

![](https://ws2.sinaimg.cn/large/006tKfTcly1g0f4tnhak8j30ck0akwes.jpg)





### LSTM的几个关键“门”与操作

![](https://ws1.sinaimg.cn/large/006tKfTcly1g0f4uq1avdj30wo0hy787.jpg)



![ ](https://ws2.sinaimg.cn/large/006tKfTcly1g0f4vgd1i5j30xq0j4wj2.jpg)

![](https://ws3.sinaimg.cn/large/006tKfTcly1g0f4wecss9j30zs0jojw1.jpg)

![](https://ws1.sinaimg.cn/large/006tKfTcly1g0f4xiqj31j30ym0joq8o.jpg)

### LSTM的变种

- 变种1：增加“peephole connection”;让“门”层也会接受细胞状态的输入。

  ![](https://ws2.sinaimg.cn/large/006tKfTcly1g0f5092frmj310a0by41f.jpg)

- 变种2：使用coupled忘记和输入门；之前是分开确定需要忘记和添加的信息，这里是一同做决定。

  ![](https://ws3.sinaimg.cn/large/006tKfTcly1g0f51hop6rj30yo0bc0uj.jpg)

## GRU(Gated Recurrent Unit)

GRU是2014年提出来的。

特点：将`遗忘门`和`输入门`合成`更新门`;同样还混合了细胞状态和隐藏状态，和其他改动；比标准lstm简单。

### GRU结构

![image-20190222130845664](/Users/wujiaoyu/Library/Application Support/typora-user-images/image-20190222130845664.png)

## 小结

- LSTM解决RNN的长时依赖问题
