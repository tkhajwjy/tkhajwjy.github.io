---

layout:     post
title:      CNN
subtitle:   CNN基础知识
date:       2018-11-24
author:     WJY
header-img: img/01.jpg
catalog: true
mathjax: true
tags:

- 深度学习


---
## 概述

**卷积神经网络(Convolutional Neural Network, CNN)**是深度学习技术中极具代表的网络结构之一，在图像处理领域取得了很大的成功，在国际标准的ImageNet数据集上，许多成功的模型都是基于CNN的。CNN相较于传统的图像处理算法的优点之一在于，降低了对图像数据预处理的要求，以及避免复杂特征工程，可以直接使用图像的原始像素作为输入。CNN最大的特点在于卷积的`权值共享`结构，可以大幅减少神经网络的参数量，防止过拟合的同时又降低神经网络模型的复杂度。

图像处理中，往往会将图像看成是一个或多个的二维向量，如MNIST手写体图片就可以看做是一个28 × 28的二维向量（黑白图片，只有一个颜色通道；如果是RGB表示的彩色图片则有三个颜色通道，可表示为三张二维向量）。传统的神经网络都是采用全连接的方式，即输入层到隐藏层的神经元都是全部连接的，这样做将导致参数量巨大，使得网络训练耗时甚至难以训练，而CNN则通过**局部连接**、**权值共享**等方法避免这一困难，有趣的是，这些方法都是受到现代生物神经网络相关研究的启发。

## 局部连接和权值共享

下图左边是全连接，右边是局部连接。

![](https://ws2.sinaimg.cn/large/006tKfTcly1g0kxfnc2mfj311w0ewtpy.jpg)

对于一个1000 × 1000的输入图像而言，如果下一个隐藏层的神经元数目为$10^{6}$个，采用全连接则有$1000 × 1000 × 10^{6 }= 10^{12}$个权值参数，如此数目巨大的参数几乎难以训练；而采用局部连接，隐藏层的每个神经元仅与图像中10 × 10的局部图像相连接，那么此时的权值参数数量为10 × 10 × 10^6 = 10^8，将直接减少4个数量级。

尽管减少了几个数量级，但参数数量依然较多。能不能再进一步减少呢？能！方法就是**权值共享**。具体做法是，在局部连接中隐藏层的每一个神经元连接的是一个10 × 10的局部图像，因此有10 × 10个权值参数，**将这10 × 10个权值参数共享给剩下的神经元，也就是说隐藏层中10^6个神经元的权值参数相同**，那么此时不管隐藏层神经元的数目是多少，需要训练的参数就是这 **10 × 10个权值参数**（也就是卷积核(也称滤波器)的大小），如下图：

![image-20190227134233822](/Users/wujiaoyu/Library/Application Support/typora-user-images/image-20190227134233822.png)

这大概就是CNN的一个神奇之处，尽管只有这么少的参数，依旧有出色的性能。但是，这样仅提取了图像的一种特征，如果要多提取出一些特征，可以增加多个卷积核，不同的卷积核能够得到图像的不同映射下的特征，称之为**Feature Map**。如果有100个卷积核，最终的权值参数也仅为$100 × 100 = 10^{4}$个而已。另外，偏置参数也是共享的，同一种滤波器共享一个。

## CNN结构

![](https://ws2.sinaimg.cn/large/006tKfTcly1g0l1585yndj31qa0u0u0x.jpg)

每隔几个卷积就有一个pooling层。最常见的卷积神经网络结构如下：

`INPUT -> [[CONV -> RELU]*N -> POOL?]*M -> [FC -> RELU]*K -> FC->softmax`，其中N最多是5，M是卷积深度，可以很大，K在0～2之间。

CNN中主要有两种类型的网络层，分别是**卷积层**、**池化/采样层(Pooling)**和**全连接层FC**。卷积层的作用是提取图像的各种特征；池化层的作用是对原始特征信号进行抽象，从而大幅度减少训练参数，另外还可以减轻模型过拟合的程度。

## 卷积层

卷积层和全连接层区别：可以保存原图空间结构spatial structure

![](https://ws3.sinaimg.cn/large/006tKfTcly1g0l07c7kv4j31t10u0dlw.jpg)

![](https://ws4.sinaimg.cn/large/006tKfTcly1g0l05qz2hnj31ol0u044r.jpg)

`note:filter的通道数和原图一样`

![](https://ws1.sinaimg.cn/large/006tKfTcly1g0l0fl380tj31so0sujxu.jpg)

卷积核中的每一个参数都相当于传统神经网络中的权值参数，与对应的局部像素相连接，将卷积核的各个参数与对应的局部像素值做点乘，通常还要再加上一个偏置参数，得到卷积层上的结果$w^{T}x+b​$。

![](https://ws1.sinaimg.cn/large/006tKfTcly1g0l0f1ltg8j31rc0tcaex.jpg)



卷积之后要加上激活层（ReLu）得到激活map。上图的步长是1，卷积之后得到28 * 28 * 1的矩阵。`如果我们使用N个卷积核，输出的激活map维度为28 * 28 * N`。

![](https://ws4.sinaimg.cn/large/006tKfTcly1g0l0sik38dj31ms0u00xn.jpg)

卷积动图如下：

这里没有加偏置项

![](http://i.imgur.com/KPyqPOB.gif)

## stride应该是多少合适呢？

![](https://ws4.sinaimg.cn/large/006tKfTcly1g0l17jpaxsj31xk0sqq7b.jpg)

N是输入的维度值，F是卷积核filter维度值。如果N=7，F=3，那么stride不能为3，因为不能拟合图像。

![](https://ws1.sinaimg.cn/large/006tKfTcly1g0l20zw0ewj31px0u07d8.jpg)

通常我们使用zero-padding，为了保持输出维度大小不变，令$P=(F-1)/2$

![](https://ws2.sinaimg.cn/large/006tKfTcly1g0l26jh2g7j31b00mo798.jpg)



![](https://ws1.sinaimg.cn/large/006tKfTcly1g0l27ofsm0j31um0lo7ap.jpg)

不要忘了乘通道数还有加上偏置项

## 卷积层小结

![](https://ws2.sinaimg.cn/large/006tKfTcly1g0l3btk4ubj31bc0jsjvu.jpg)

假设输入层维度是$W_{1}*H_{1}*D_{1}$

超参数：卷积核个数K（一般是$2^{n}$,如32、64）、卷积核大小F（一般$3*3、5*5$）、步长S(一般1或2)、0填充大小P

则输出层维度是$W_{2}*H_{2}*D_{2}$，其中$W_{2}=(W_{1}+2P-F)/S+1$,$H_{2}=(H_{1}+2P-F)/S+1$,$D_{2}=K$

这里面超参数有多少个？

由于权值共享：每个卷积核有$（F*F*D_{1}+1）$个参数,K个卷积核有$(F*F*D_{1}+1）*K$个参数

在输出层，第k层表示在第k个filter作用下输出结果



## 池化层/采样层

通过卷积层获得了图像的特征之后，理论上我们可以直接使用这些特征训练分类器（如softmax），但是这样做将面临巨大的计算量的挑战，而且容易产生过拟合的现象。为了进一步降低网络训练参数及模型的过拟合程度，我们对卷积层进行**池化/采样(Pooling)**处理。池化/采样的方式通常有以下两种：

- `Max-Pooling:`选择Pooling窗口最大值作为采样（常用方法）。

- `Mean-Pooling:`将Pooling窗口的所有值相加后取平均，平均值作为采样值。

  

  ![](https://ws2.sinaimg.cn/large/006tKfTcly1g0kxqnzhalj31c20dm11e.jpg)



pooling是独立对每个`activation map`做处理，所以深度不变。pooling也用到filter，一般是不希望区域有重叠部分

![](https://ws2.sinaimg.cn/large/006tKfTcly1g0l3azhirkj314k0eg76t.jpg)

一般不在pooling层做zero-padding，因为pooling层只希望做降采样，这样卷积核扫过边缘时候不会有超出输入范围的部分。一般filter为2*2，步长为2。

## FC层

在全连接层中，神经元对于前一层中的所有激活数据是全部连接的，这个常规神经网络中一样。

## 小结

- 为什么要共享卷积核的权值参数？降低模型复杂度、减少过拟合、减少计算量。

- 小尺寸的filter和更深的网络结构是趋势。

- 去除pooling/FC层只保留卷积层是趋势。

- 典型CNN结构：`INPUT -> [[CONV -> RELU]*N -> POOL?]*M -> [FC -> RELU]*K -> FC->softmax`

  其中N最多是5，M是卷积深度，可以很大，K在0～2之间。

  

  

  
